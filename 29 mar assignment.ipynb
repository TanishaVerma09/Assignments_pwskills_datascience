{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d6c318-0ec0-4fa8-aa79-69466c430da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1\n",
    "# What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcf2bf-a531-4688-926e-938762fd2e0b",
   "metadata": {},
   "source": [
    "Lasso Regression, short for \"Least Absolute Shrinkage and Selection Operator\" regression, is a regularization technique used in linear regression to prevent overfitting and improve the model's generalization performance. It achieves this by adding a penalty term to the linear regression's objective function, which encourages the model to minimize the absolute values of the coefficients.\n",
    "\n",
    "In standard linear regression, the goal is to find the coefficients of the independent variables that best fit the training data, minimizing the sum of squared differences between the predicted and actual target values. However, this can lead to overfitting when the model becomes too complex, capturing noise in the data and resulting in poor performance on new, unseen data.\n",
    "\n",
    "Lasso Regression introduces a penalty term to the linear regression's objective function:\n",
    "\n",
    "Objective function with Lasso penalty:\n",
    "\n",
    "makefile\n",
    "Copy code\n",
    "Minimize: (1/2) * ||Y - Xβ||^2 + λ * ||β||_1\n",
    "Where:\n",
    "\n",
    "Y is the vector of target values.\n",
    "X is the matrix of independent variables.\n",
    "β is the vector of coefficients being optimized.\n",
    "λ (lambda) is the regularization parameter that controls the strength of the penalty.\n",
    "||·||^2 represents the L2 (Euclidean) norm, and ||·||_1 represents the L1 norm.\n",
    "The Lasso penalty term (λ * ||β||_1) has the effect of shrinking some coefficients to exactly zero. This leads to feature selection, meaning that some independent variables become irrelevant in the model, effectively removing them. This is a key characteristic of Lasso Regression and sets it apart from other regression techniques, particularly from Ridge Regression.\n",
    "\n",
    "The key differences between Lasso Regression and other regression techniques, such as Ridge Regression, are:\n",
    "\n",
    "L1 vs. L2 Penalty: Lasso Regression uses an L1 penalty, which encourages sparsity in the coefficient vector by pushing some coefficients to zero. In contrast, Ridge Regression uses an L2 penalty, which shrinks the coefficients towards zero but rarely exactly to zero, keeping all features in the model.\n",
    "\n",
    "Feature Selection: Lasso Regression performs automatic feature selection by driving some coefficients to zero. This can be useful when dealing with datasets with many irrelevant or redundant features, as it simplifies the model and may improve its interpretability.\n",
    "\n",
    "Solution Path: As the value of the regularization parameter λ increases, some coefficients in Lasso Regression are driven to zero one by one, leading to a \"path\" of solutions. This can help in identifying the most important features.\n",
    "\n",
    "Bias-Variance Trade-off: Lasso Regression tends to perform well when dealing with a dataset with a large number of features, as it can effectively shrink less important coefficients to zero, reducing overfitting. However, if the number of features is small, Lasso might not significantly outperform other regression techniques.\n",
    "\n",
    "In summary, Lasso Regression is a valuable tool for both feature selection and regularization, and it is particularly useful when dealing with high-dimensional datasets where feature selection is important.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57bf09c-d005-4f3e-87b6-d34a45772c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2\n",
    "# What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83223fc-119c-4ec1-92ba-f54809ca6aa5",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression for feature selection is its ability to automatically identify and select relevant features while driving irrelevant features to exactly zero. This is particularly valuable when dealing with high-dimensional datasets that have a large number of features, many of which may be irrelevant, redundant, or noise.\n",
    "\n",
    "Here are some key advantages of using Lasso Regression for feature selection:\n",
    "\n",
    "Automatic Feature Selection: Lasso Regression performs automatic feature selection by assigning a weight of zero to irrelevant features. This eliminates the need for manual feature selection, which can be time-consuming and prone to human bias.\n",
    "\n",
    "Simplicity: By setting the coefficients of some features to zero, Lasso simplifies the model and reduces its complexity. This can lead to a more interpretable and understandable model, as well as potentially improved generalization performance on new data.\n",
    "\n",
    "Handling Multicollinearity: Lasso Regression can handle multicollinearity (high correlation between features) effectively by selecting one of the correlated features and pushing the coefficients of the others to zero. This helps in reducing the risk of overfitting caused by multicollinearity.\n",
    "\n",
    "Improved Generalization: Removing irrelevant features through Lasso Regression reduces the risk of overfitting, leading to better generalization performance on unseen data. This is particularly important when working with limited data.\n",
    "\n",
    "Identifying Important Features: The path of solutions produced by Lasso as the regularization parameter varies can help identify the most important features. This can give insights into the relationships between features and the target variable.\n",
    "\n",
    "Sparse Models: Lasso tends to produce sparse models, meaning that only a subset of the original features are retained. Sparse models are computationally efficient and easier to work with, especially when deploying the model in real-world applications.\n",
    "\n",
    "Preventing Data Leakage: Feature selection using Lasso helps prevent data leakage, a situation where irrelevant or redundant features accidentally contribute to the model's performance on new data, leading to overly optimistic evaluations.\n",
    "\n",
    "It's important to note that while Lasso Regression offers these advantages, it's not always the best choice. For example, if you suspect that all features are potentially relevant and you don't want any coefficients to be exactly zero, Ridge Regression might be a better choice. The choice between Lasso, Ridge, or other regularization techniques should depend on the specific characteristics of your data and the goals of your analysis or modeling task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21349f5c-00a6-4d66-b8a1-8b3aa2e41519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3\n",
    "# How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046064d-affd-47e2-b9d7-90c6d15a28da",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model is slightly different from interpreting coefficients in a standard linear regression model due to the nature of Lasso's regularization. In Lasso Regression, the coefficients are influenced by both the data and the regularization term, which can result in some coefficients being exactly zero. Here's how you can interpret the coefficients in a Lasso Regression model:\n",
    "\n",
    "Non-Zero Coefficients: The non-zero coefficients indicate the strength and direction of the relationship between each feature and the target variable, similar to linear regression. A positive coefficient suggests a positive correlation between the feature and the target, meaning an increase in the feature's value corresponds to an increase in the target's predicted value, and vice versa. A negative coefficient indicates a negative correlation.\n",
    "\n",
    "Zero Coefficients: Coefficients that are exactly zero indicate that the corresponding features have been deemed irrelevant by the Lasso regularization. These features have been effectively removed from the model. The fact that a coefficient is zero can be seen as a form of feature selection, implying that the associated feature has no impact on the predicted outcome.\n",
    "\n",
    "Magnitude of Coefficients: The magnitudes of the non-zero coefficients give you an idea of the strength of the relationship between each feature and the target. Larger magnitudes indicate stronger influence. However, comparing the magnitudes of coefficients between features might not always provide a fair comparison, as Lasso can shrink some coefficients more aggressively than others.\n",
    "\n",
    "Direction of Coefficients: Just like in linear regression, the sign (positive or negative) of a coefficient indicates the direction of the relationship between the feature and the target. Positive coefficients imply a positive effect on the target, while negative coefficients imply a negative effect.\n",
    "\n",
    "Regularization Effect: It's important to remember that the coefficients in a Lasso model are influenced by both the data fitting and the regularization term. As the regularization parameter increases, more coefficients tend to be pushed towards zero. Therefore, the actual values of the coefficients can depend on the choice of the regularization parameter.\n",
    "\n",
    "Feature Importance: If you're interested in assessing feature importance, Lasso Regression naturally highlights important features by assigning non-zero coefficients to them. Features with non-zero coefficients have survived the regularization process and are considered more influential in predicting the target.\n",
    "\n",
    "Interpretation Challenges: Keep in mind that interpreting coefficients becomes more complex as the model's complexity increases, especially when dealing with multicollinearity (high correlation between features) or interactions between features. Additionally, the practical significance of a coefficient might differ from its statistical significance.\n",
    "\n",
    "In summary, interpreting the coefficients of a Lasso Regression model involves understanding both the direction and magnitude of the coefficients, identifying zero coefficients that correspond to removed features, and recognizing the impact of regularization on the coefficient values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f17b270-d869-4e0e-855c-c0dedfb9bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4\n",
    "# What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff98dfb-844f-4baa-974a-3845ffeaf59d",
   "metadata": {},
   "source": [
    "In Lasso Regression, there are primarily two tuning parameters that can be adjusted to control the model's performance: the regularization parameter (λ, lambda) and the choice of feature scaling. These parameters have a significant impact on how the Lasso Regression model behaves and performs.\n",
    "\n",
    "Regularization Parameter (λ): The regularization parameter controls the strength of the L1 penalty in the Lasso Regression objective function. It determines the trade-off between fitting the data well and keeping the coefficients small. Higher values of λ result in more aggressive regularization, leading to more coefficients being pushed towards exactly zero. Lower values of λ allow the model to fit the training data more closely, potentially resulting in overfitting.\n",
    "\n",
    "Effect on Model Complexity: As λ increases, the model becomes simpler by shrinking more coefficients to zero. This can help prevent overfitting and improve generalization.\n",
    "\n",
    "Feature Selection: Higher λ values lead to more features being excluded from the model, effectively performing feature selection. This can be particularly useful when dealing with high-dimensional datasets with many irrelevant features.\n",
    "\n",
    "Finding the Optimal λ: The optimal value of λ is often chosen using techniques like cross-validation. Cross-validation involves training the model on different subsets of the training data and evaluating its performance on validation data. The value of λ that results in the best validation performance is selected.\n",
    "\n",
    "Feature Scaling: Feature scaling refers to the process of standardizing or normalizing the feature variables before applying Lasso Regression. Feature scaling is important in Lasso because the penalty term (λ * ||β||_1) treats all coefficients equally. If the features have different scales, those with larger scales might dominate the regularization process.\n",
    "\n",
    "Effect of Feature Scaling: When features are not scaled, features with larger magnitudes can have a disproportionate impact on the regularization process. Scaling the features ensures that the regularization is applied fairly across all features, leading to a more balanced influence on the model's coefficients.\n",
    "In summary, the tuning parameters in Lasso Regression, namely the regularization parameter (λ) and feature scaling, play crucial roles in controlling the model's complexity, handling overfitting, performing feature selection, and ensuring fair regularization across features with different scales. The appropriate choice of these parameters depends on the specific characteristics of the data, the desired level of complexity in the model, and the trade-off between accuracy and interpretability. Experimentation and techniques like cross-validation can help determine the optimal values for these parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a8d63c-d8f5-48f0-9299-f1f069746d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5\n",
    "# Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da72aa7-d883-4e84-8996-771290027acb",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can be extended to handle non-linear regression problems through a technique called \"Lasso Regression with Polynomial Features\" or \"Polynomial Lasso Regression.\" This approach involves transforming the original features into polynomial features before applying Lasso Regression. By introducing polynomial features, the model can capture non-linear relationships between the features and the target variable.\n",
    "\n",
    "Here's how you can use Lasso Regression for non-linear regression problems:\n",
    "\n",
    "Feature Transformation: If you have a non-linear relationship between your features and the target variable, you can create polynomial features by raising the original features to different powers (e.g., squares, cubes) and possibly creating interaction terms. This transformation allows the model to capture more complex relationships.\n",
    "\n",
    "Apply Lasso Regression: Once you've transformed the features into polynomial features, you can apply Lasso Regression as you would in a linear regression setting. The Lasso penalty term will encourage some of the coefficients associated with the polynomial features to be exactly zero, effectively performing feature selection and simplifying the model.\n",
    "\n",
    "Hyperparameter Tuning: You will still need to choose the appropriate value for the regularization parameter (λ) in the Lasso Regression. This can be done using techniques like cross-validation to find the optimal balance between fitting the data well and preventing overfitting.\n",
    "\n",
    "It's important to note a few considerations when using Lasso Regression for non-linear regression:\n",
    "\n",
    "Feature Engineering: Creating polynomial features can lead to a high-dimensional feature space, especially when using higher-degree polynomial terms. This can result in increased model complexity and potentially lead to overfitting, so careful feature selection and regularization are important.\n",
    "\n",
    "Polynomial Degree: The degree of the polynomial features to use is a hyperparameter that needs to be determined. Higher-degree polynomials can fit complex non-linear relationships, but they can also lead to overfitting, especially when the dataset is small.\n",
    "\n",
    "Interpretation: Interpretation becomes more challenging in non-linear models, as the relationship between the original features and the target variable is obscured by the polynomial transformation. Understanding the impact of each feature on the target can be less intuitive compared to linear models.\n",
    "\n",
    "Regularization: Lasso Regression's regularization can still help prevent overfitting in polynomial models, but finding the right level of regularization becomes even more crucial due to the increased complexity.\n",
    "\n",
    "In summary, Lasso Regression can be used for non-linear regression problems by transforming the features into polynomial features to capture non-linear relationships. However, careful consideration of feature engineering, polynomial degree, regularization, and interpretation is necessary to ensure effective model performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ec741b-3c63-4e4a-986d-6455364597a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6\n",
    "# What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeaaa50-53a4-4e8f-9a86-346a5839007e",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both regularization techniques used in linear regression to prevent overfitting and improve the model's generalization performance. They achieve this by adding penalty terms to the linear regression's objective function. While they share similarities, they have distinct differences in how they apply regularization and handle feature selection.\n",
    "\n",
    "Here are the main differences between Ridge Regression and Lasso Regression:\n",
    "\n",
    "Penalty Type:\n",
    "\n",
    "Ridge Regression: Ridge Regression uses an L2 (Euclidean) penalty term. The penalty term is the sum of the squares of the coefficients, multiplied by a regularization parameter (λ). It encourages the coefficients to be small but rarely exactly zero.\n",
    "Lasso Regression: Lasso Regression uses an L1 penalty term. The penalty term is the sum of the absolute values of the coefficients, multiplied by a regularization parameter (λ). It can drive some coefficients exactly to zero, effectively performing feature selection.\n",
    "Feature Selection:\n",
    "\n",
    "Ridge Regression: Ridge Regression tends to shrink the coefficients towards zero without setting them exactly to zero. This means that all features are retained in the model, and none are completely excluded.\n",
    "Lasso Regression: Lasso Regression can set some coefficients to exactly zero. This leads to automatic feature selection, as some features are removed from the model. Lasso is particularly useful when dealing with datasets with many irrelevant features.\n",
    "Solution Path:\n",
    "\n",
    "Ridge Regression: As the regularization parameter (λ) increases in Ridge Regression, the coefficients gradually shrink towards zero, but they rarely become exactly zero.\n",
    "Lasso Regression: As the regularization parameter (λ) increases in Lasso Regression, some coefficients are driven to exactly zero one by one. This results in a \"path\" of solutions, which can help identify important features.\n",
    "Handling Multicollinearity:\n",
    "\n",
    "Both Ridge and Lasso Regression can help handle multicollinearity (high correlation between features) to some extent. However, Ridge Regression is generally better suited for this task because it distributes the impact of correlated features more evenly across coefficients, while Lasso may arbitrarily choose one of the correlated features and push the others to zero.\n",
    "Number of Features:\n",
    "\n",
    "Ridge Regression: It usually keeps all features in the model but reduces their impact by shrinking coefficients.\n",
    "Lasso Regression: It can lead to a sparse model with only a subset of features having non-zero coefficients, effectively performing feature selection.\n",
    "Interpretability:\n",
    "\n",
    "Ridge Regression: Coefficients are generally not exactly zero, which might make interpretation easier in some cases.\n",
    "Lasso Regression: Coefficients can be exactly zero, providing clear indications of which features are important and which are irrelevant.\n",
    "In summary, Ridge Regression and Lasso Regression offer different approaches to regularization and feature selection. Ridge tends to shrink coefficients towards zero without excluding them, while Lasso can drive some coefficients exactly to zero, effectively selecting features. The choice between Ridge and Lasso, or a combination of both (Elastic Net), depends on the characteristics of the data and the desired model behavior.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa2dc3f-c6ed-4ddb-b6c9-18a1ee2d5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 7\n",
    "# Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4773b-4bf3-427e-ae5e-3d6d95e9edb9",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can help mitigate multicollinearity to some extent, although it does so in a different way compared to Ridge Regression. Multicollinearity is a situation where two or more independent variables in a regression model are highly correlated with each other, which can cause instability in the coefficient estimates and affect the model's interpretability.\n",
    "\n",
    "Here's how Lasso Regression handles multicollinearity:\n",
    "\n",
    "Coefficient Shrinkage: Lasso Regression introduces an L1 (absolute value) penalty term in the objective function, which has the effect of shrinking some coefficients towards zero. This penalty encourages sparsity by driving some coefficients to exactly zero. When features are highly correlated due to multicollinearity, Lasso might arbitrarily choose one of the correlated features and drive its coefficient to zero, effectively excluding it from the model.\n",
    "\n",
    "Feature Selection: Since Lasso Regression can set coefficients to exactly zero, it automatically performs feature selection by removing certain features from the model. When there is multicollinearity, Lasso may select one of the correlated features and exclude others, effectively addressing the multicollinearity issue by ignoring one of the correlated variables.\n",
    "\n",
    "Less Pronounced Effect: Lasso's impact on multicollinearity is generally less pronounced compared to Ridge Regression. In Ridge Regression, the L2 penalty aims to distribute the impact of correlated features more evenly across coefficients, leading to smaller but non-zero coefficients for all features. Lasso's L1 penalty, on the other hand, can completely exclude some correlated features from the model.\n",
    "\n",
    "However, it's important to note that while Lasso Regression can help with multicollinearity to some degree, it might not be the best choice in all situations. Here are a few points to consider:\n",
    "\n",
    "Feature Selection Bias: The features excluded by Lasso due to multicollinearity might not necessarily be irrelevant or redundant. Lasso's selection might be based on chance, and important features could be lost.\n",
    "\n",
    "Ridge Regression Alternative: Ridge Regression is often considered more suitable for dealing with multicollinearity, as it redistributes the impact of correlated features across coefficients, maintaining all features in the model while reducing their impact.\n",
    "\n",
    "Elastic Net: If multicollinearity is a significant concern, Elastic Net regression, which combines L1 and L2 penalties, can be a useful compromise. It can provide the benefits of both Lasso and Ridge, handling multicollinearity and feature selection simultaneously.\n",
    "\n",
    "In summary, while Lasso Regression can handle multicollinearity by excluding some correlated features, Ridge Regression or Elastic Net might be better suited to more effectively manage the impact of correlated features while retaining all features in the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6fa4618-9ff4-4872-acd8-88a4192a6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 8\n",
    "# How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3ef96-a48a-416e-b740-e378800d8952",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (λ, lambda) in Lasso Regression involves finding a balance between fitting the training data well and preventing overfitting. Cross-validation is a common technique used to determine the appropriate value of λ. The basic idea is to evaluate the model's performance on different subsets of the training data to select the regularization parameter that yields the best generalization performance.\n",
    "\n",
    "Here's a step-by-step approach to choosing the optimal λ in Lasso Regression using cross-validation:\n",
    "\n",
    "Divide Data: Split your dataset into three subsets: training set, validation set, and test set. The training set is used to train the models, the validation set is used to tune the regularization parameter, and the test set is used to evaluate the final model's performance.\n",
    "\n",
    "Create a Range of λ Values: Choose a range of possible λ values to evaluate. These values typically cover a wide range from very small values (almost no regularization) to very large values (strong regularization).\n",
    "\n",
    "Loop over λ Values: For each value of λ in your chosen range, do the following steps:\n",
    "\n",
    "a. Fit a Lasso Regression model on the training data using the current λ value.\n",
    "\n",
    "b. Evaluate the model's performance on the validation set. You can use a suitable performance metric, such as Mean Squared Error (MSE) or R-squared.\n",
    "\n",
    "Select Optimal λ: Choose the λ value that resulted in the best performance on the validation set. This can be the value that minimized the chosen performance metric (e.g., lowest MSE or highest R-squared).\n",
    "\n",
    "Evaluate on Test Set: After selecting the optimal λ, use this value to train a Lasso Regression model on the entire training dataset (training set + validation set). Then, evaluate the model's performance on the independent test set to get an unbiased estimate of its generalization performance.\n",
    "\n",
    "Additional Considerations: You might also consider techniques like k-fold cross-validation, where the dataset is divided into k subsets (folds), and the model is trained and validated on different combinations of these folds. This provides a more robust estimate of the model's performance.\n",
    "\n",
    "Python's scikit-learn library provides tools to perform cross-validation easily, such as GridSearchCV or RandomizedSearchCV, which automate the process of trying different hyperparameters and selecting the best one based on cross-validation.\n",
    "\n",
    "Keep in mind that the optimal value of λ can depend on the specific characteristics of your dataset, so it's a good practice to experiment with different ranges of λ values and perform thorough cross-validation to make an informed choice.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f4aef-fb7f-4e0d-a21e-c5a8d11e1ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
