{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3553393-33d4-4249-b903-b47fb5bb7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec79004-ec2d-4f35-9346-980260aa99e9",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform the values of a dataset into a specific range, typically between 0 and 1. This scaling method is particularly useful when features in your dataset have different scales and you want to bring them all into a consistent range to prevent certain features from dominating the learning process of machine learning algorithms that are sensitive to the scale of input features.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "scaled\n",
    "X \n",
    "scaled\n",
    "​\n",
    "  is the scaled value of the data point \n",
    "�\n",
    "X\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value in the feature's original range\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value in the feature's original range\n",
    "After applying Min-Max scaling, the transformed values will be in the range [0, 1], where 0 represents the minimum value and 1 represents the maximum value of the original feature.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have a dataset containing two features, \"Age\" and \"Income\", and you want to apply Min-Max scaling to bring both features into the range [0, 1].\n",
    "\n",
    "Original dataset (sample):\n",
    "\n",
    "Age\tIncome\n",
    "30\t50000\n",
    "40\t75000\n",
    "25\t60000\n",
    "35\t90000\n",
    "28\t55000\n",
    "To apply Min-Max scaling to the \"Age\" feature:\n",
    "\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  (minimum Age) = 25\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  (maximum Age) = 40\n",
    "For the first data point (Age = 30):\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "30\n",
    "−\n",
    "25\n",
    "40\n",
    "−\n",
    "25\n",
    "=\n",
    "0.333\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "40−25\n",
    "30−25\n",
    "​\n",
    " =0.333\n",
    "\n",
    "Applying the same formula to all other data points in the \"Age\" feature, you'll get scaled values between 0 and 1.\n",
    "\n",
    "Similarly, for the \"Income\" feature:\n",
    "\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  (minimum Income) = 50000\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  (maximum Income) = 90000\n",
    "After applying Min-Max scaling to both features, your transformed dataset might look like this:\n",
    "\n",
    "Age (scaled)\tIncome (scaled)\n",
    "0.333\t0.222\n",
    "1.000\t0.556\n",
    "0.000\t0.333\n",
    "0.667\t1.000\n",
    "0.167\t0.111\n",
    "This process ensures that both features are now in the same scale range, making it easier for machine learning algorithms to work effectively with the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6f62f5-4586-4111-9fb1-f4ea19e992cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513caec-8b4d-41eb-beb0-07dd19ee81a6",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or unit norm scaling, is a feature scaling method that transforms the values of each data point in a dataset to have a unit norm or length of 1 while preserving the direction of the original data. This technique is commonly used when the direction of the data points is more important than their magnitude. Unit Vector scaling is especially useful when working with algorithms that rely on the cosine similarity between data points.\n",
    "\n",
    "Mathematically, the Unit Vector scaling is applied as follows:\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "∥\n",
    "�\n",
    "∥\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "∥X∥\n",
    "X\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "scaled\n",
    "X \n",
    "scaled\n",
    "​\n",
    "  is the scaled vector of the data point \n",
    "�\n",
    "X\n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥X∥ is the Euclidean norm (length) of the original vector \n",
    "�\n",
    "X\n",
    "Unlike Min-Max scaling, which brings the data into a specific range, Unit Vector scaling focuses on transforming the data points so that they have a consistent direction (unit length) without changing their relative distances from the origin.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a dataset with two features, \"Height\" and \"Weight\". We want to apply Unit Vector scaling to the data points.\n",
    "\n",
    "Original dataset (sample):\n",
    "\n",
    "Height\tWeight\n",
    "160\t50\n",
    "175\t70\n",
    "150\t45\n",
    "180\t75\n",
    "165\t55\n",
    "To apply Unit Vector scaling to a data point (e.g., [160, 50]):\n",
    "\n",
    "Calculate the Euclidean norm (\n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥X∥) of the original vector:\n",
    "∥\n",
    "�\n",
    "∥\n",
    "=\n",
    "16\n",
    "0\n",
    "2\n",
    "+\n",
    "5\n",
    "0\n",
    "2\n",
    "≈\n",
    "167.8\n",
    "∥X∥= \n",
    "160 \n",
    "2\n",
    " +50 \n",
    "2\n",
    " \n",
    "​\n",
    " ≈167.8\n",
    "\n",
    "Divide each component of the vector by its Euclidean norm to get the scaled vector:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "[\n",
    "160\n",
    "167.8\n",
    ",\n",
    "50\n",
    "167.8\n",
    "]\n",
    "≈\n",
    "[\n",
    "0.953\n",
    ",\n",
    "0.298\n",
    "]\n",
    "X \n",
    "scaled\n",
    "​\n",
    " =[ \n",
    "167.8\n",
    "160\n",
    "​\n",
    " , \n",
    "167.8\n",
    "50\n",
    "​\n",
    " ]≈[0.953,0.298]\n",
    "\n",
    "Repeat this process for all data points in the dataset to obtain the scaled vectors.\n",
    "\n",
    "After applying Unit Vector scaling, your transformed dataset might look like this (rounded to three decimal places):\n",
    "\n",
    "Height (scaled)\tWeight (scaled)\n",
    "[0.953, 0.298]\t[0.747, 0.665]\n",
    "[0.985, 0.174]\t[0.696, 0.718]\n",
    "[0.970, 0.244]\t[0.737, 0.676]\n",
    "[0.980, 0.197]\t[0.717, 0.697]\n",
    "[0.974, 0.226]\t[0.756, 0.655]\n",
    "In this example, the Unit Vector scaling has transformed each data point to have a unit length while maintaining their direction in the original feature space. This is useful when the magnitude of the features is less important than their relative orientations.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff5eda8-43b0-4b1b-9982-5da01c90dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d10d6-0cd6-4241-85cc-d25903c93d74",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while preserving as much of the original data's variability as possible. It does this by finding a set of new orthogonal axes (principal components) that capture the maximum variance in the data. These principal components are linear combinations of the original features, and they are ordered in such a way that the first component captures the most variance, the second component captures the second most variance, and so on.\n",
    "\n",
    "PCA is commonly used for various purposes, including data visualization, noise reduction, and improving the efficiency of machine learning algorithms by reducing the number of features.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's illustrate PCA with a simple example using a 2D dataset:\n",
    "\n",
    "Original dataset (sample):\n",
    "\n",
    "Feature 1\tFeature 2\n",
    "1.2\t2.3\n",
    "2.5\t0.5\n",
    "0.3\t0.8\n",
    "2.8\t2.9\n",
    "0.5\t1.5\n",
    "Standardize the data:\n",
    "Calculate mean and standard deviation for each feature and standardize the data.\n",
    "\n",
    "Calculate the covariance matrix:\n",
    "Compute the covariance matrix based on the standardized data.\n",
    "\n",
    "Compute eigenvectors and eigenvalues:\n",
    "Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort eigenvalues:\n",
    "Sort the eigenvalues in descending order.\n",
    "\n",
    "Select principal components:\n",
    "Choose the top principal component (first eigenvector) to reduce the dimensionality to 1.\n",
    "\n",
    "Project data onto new components:\n",
    "Transform the original data onto the new 1D space defined by the first principal component.\n",
    "\n",
    "After performing PCA, your transformed dataset might look like this:\n",
    "\n",
    "Principal Component 1\n",
    "2.009\n",
    "-0.609\n",
    "-0.818\n",
    "2.815\n",
    "0.603\n",
    "In this example, PCA has reduced the original 2D dataset to a 1D representation while preserving as much of the variability in the data as possible. The new feature represents the direction of maximum variance in the original data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1e82f9-fb79-4e99-8ca9-5d04f119ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d4942-730c-40cb-a595-3ec3440c363f",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique that can also be used for feature extraction. While dimensionality reduction aims to reduce the number of features while preserving most of the variance, feature extraction involves transforming the original features into a new set of features that capture the most relevant information in the data. PCA achieves both goals simultaneously by identifying the principal components, which are linear combinations of the original features that capture the maximum variance and therefore the most relevant information.\n",
    "\n",
    "In the context of feature extraction, PCA works as follows:\n",
    "\n",
    "Standardize the data: Just like in dimensionality reduction, you start by standardizing the data to have zero mean and unit variance.\n",
    "\n",
    "Calculate principal components: PCA identifies the principal components by computing the eigenvectors and eigenvalues of the covariance matrix of the standardized data.\n",
    "\n",
    "Select top components: You choose a subset of the principal components based on the amount of variance they explain. These components effectively represent the most informative directions in the data.\n",
    "\n",
    "Project data onto selected components: You project the original data onto the selected principal components, creating a new set of features that captures the most important information in the data.\n",
    "\n",
    "Reduce dimensionality: If you want to reduce the dimensionality, you can choose to keep only the top \n",
    "�\n",
    "k principal components, effectively reducing the number of features.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's illustrate how PCA can be used for feature extraction with a simple example:\n",
    "\n",
    "Original dataset (sample):\n",
    "\n",
    "Feature 1\tFeature 2\tFeature 3\tFeature 4\n",
    "3.5\t2.0\t4.2\t1.8\n",
    "2.1\t1.3\t2.9\t0.8\n",
    "5.2\t3.0\t5.7\t2.4\n",
    "1.9\t1.5\t3.0\t1.0\n",
    "4.7\t2.8\t5.1\t2.2\n",
    "Standardize the data.\n",
    "\n",
    "Calculate principal components:\n",
    "\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Select top components:\n",
    "Suppose we decide to keep the first two principal components as our new features since they capture most of the variance.\n",
    "\n",
    "Project data onto selected components:\n",
    "Transform the original data onto the new 2D space defined by the first two principal components.\n",
    "\n",
    "After performing PCA for feature extraction, your transformed dataset might look like this:\n",
    "\n",
    "Principal Component 1\tPrincipal Component 2\n",
    "0.697\t-0.322\n",
    "-2.015\t-0.352\n",
    "2.458\t0.540\n",
    "-1.573\t-0.125\n",
    "0.433\t0.259\n",
    "In this example, PCA has extracted two principal components, which can be considered as the new features capturing the most relevant information in the original data. These new features are orthogonal (uncorrelated) and are linear combinations of the original features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61bb61d-d46a-4182-a8d4-3342ede9c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd3581-c601-41e9-bb4d-5d970d8ab5eb",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be applied to preprocess the features of the dataset, such as price, rating, and delivery time. The goal of Min-Max scaling is to bring all the features into a common range, typically between 0 and 1, to ensure that they have similar scales and to prevent certain features from dominating the recommendation process.\n",
    "\n",
    "Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "Understand the Data: Begin by understanding the range and distribution of each feature in your dataset, such as price, rating, and delivery time. This will help you determine the appropriate scaling method to use.\n",
    "\n",
    "Standardize the Data: For Min-Max scaling, you need to standardize the data by subtracting the minimum value of each feature from its original value, and then dividing by the range (difference between maximum and minimum values) of that feature.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "scaled\n",
    "X \n",
    "scaled\n",
    "​\n",
    "  is the scaled value of the feature \n",
    "�\n",
    "X\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of the feature\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of the feature\n",
    "Apply Min-Max Scaling: Apply the Min-Max scaling formula to each feature in the dataset separately. This will transform the original values into a common scale between 0 and 1.\n",
    "\n",
    "Scaled Feature Range: Once the scaling is applied, the transformed features will have a range of [0, 1], where 0 represents the minimum value of the original feature and 1 represents the maximum value.\n",
    "\n",
    "For example, let's consider a subset of your dataset with three features: price, rating, and delivery time.\n",
    "\n",
    "Original dataset (sample):\n",
    "\n",
    "Price\tRating\tDelivery Time\n",
    "15\t4.5\t30 min\n",
    "20\t4.2\t45 min\n",
    "10\t4.8\t25 min\n",
    "25\t3.9\t60 min\n",
    "30\t4.6\t40 min\n",
    "Applying Min-Max scaling to the \"Price\" feature:\n",
    "\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  (minimum Price) = 10\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  (maximum Price) = 30\n",
    "For the first data point (Price = 15):\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "15\n",
    "−\n",
    "10\n",
    "30\n",
    "−\n",
    "10\n",
    "=\n",
    "0.5\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "30−10\n",
    "15−10\n",
    "​\n",
    " =0.5\n",
    "\n",
    "Repeat this process for the \"Rating\" and \"Delivery Time\" features, using their respective minimum and maximum values.\n",
    "\n",
    "After Min-Max scaling, your transformed dataset might look like this:\n",
    "\n",
    "Price (scaled)\tRating (scaled)\tDelivery Time (scaled)\n",
    "0.25\t0.5\t0.25\n",
    "0.375\t0.3\t0.5\n",
    "0.0\t0.8\t0.0\n",
    "0.625\t0.0\t1.0\n",
    "0.875\t0.6\t0.375\n",
    "By applying Min-Max scaling, you've transformed the original features into a consistent range between 0 and 1, which can help ensure that all features contribute equally to the recommendation system and improve the accuracy of your models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692d22b6-3586-4e1e-bce9-249e4ce0658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ff882-1d8c-4421-a523-f375b09fec91",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices using a dataset with numerous features, such as company financial data and market trends, PCA (Principal Component Analysis) can be used as a dimensionality reduction technique to simplify the dataset while retaining most of the relevant information. Here's how you would use PCA to reduce the dimensionality of the dataset for your stock price prediction model:\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Start by cleaning and preprocessing your dataset. Handle missing values, outliers, and perform any necessary data transformations.\n",
    "Standardize the data by scaling each feature to have a mean of 0 and a standard deviation of 1. This step is crucial for PCA to work effectively, as it assumes that the data is centered and has a consistent scale.\n",
    "Calculate Covariance Matrix:\n",
    "\n",
    "Compute the covariance matrix of the standardized dataset. The covariance matrix represents the relationships between the features and indicates how they vary together.\n",
    "Eigenvalue Decomposition:\n",
    "\n",
    "Calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions (principal components) in which the data varies the most, and the eigenvalues indicate the amount of variance explained by each component.\n",
    "Sort and Select Principal Components:\n",
    "\n",
    "Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvector with the highest eigenvalue captures the most variance in the data and becomes the first principal component.\n",
    "Choose a subset of the top principal components that collectively capture a significant portion of the total variance in the data. The number of components you select will determine the reduced dimensionality of the dataset.\n",
    "Project Data onto Principal Components:\n",
    "\n",
    "Project the original standardized data onto the selected principal components. This creates a new dataset in a lower-dimensional space.\n",
    "Inverse Transformation (Optional):\n",
    "\n",
    "If needed, you can perform an inverse transformation to map the data back to the original feature space. This step is useful if you want to interpret the reduced-dimensional data in terms of the original features.\n",
    "Using PCA for dimensionality reduction in your stock price prediction project offers several benefits:\n",
    "\n",
    "Reduces the number of features, which can help mitigate the curse of dimensionality and improve model efficiency.\n",
    "Removes noise and focuses on the most significant information in the data.\n",
    "Can reveal hidden patterns and relationships between features that contribute to stock price movement.\n",
    "Keep in mind that while PCA can simplify the dataset, it may also result in a loss of interpretability, as the new components might not have clear physical or semantic meanings. You should carefully evaluate the trade-offs and consider using domain knowledge when selecting the number of principal components to retain.\n",
    "\n",
    "In summary, PCA can be a valuable tool for reducing the dimensionality of your stock price prediction dataset, leading to more efficient and potentially more accurate predictive models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b439ad5-f7d3-49d6-86bf-40402b14789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa7384d-5ace-4fa0-a8da-6134ae5f6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4740c540-f4bd-4437-8c7b-c2af2c12ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([1,5,10,15,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdfbae7c-90f6-4fa3-9674-1af303451522",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value=np.min(data)\n",
    "max_value=np.max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9690474-9503-4653-a983-e5b18aa9ddf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_value,max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d91d15b-1f33-43f8-a130-9bb59b7fba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  5 10 15 20]\n",
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "scaled_data=[((x - min_value) / (max_value - min_value)) * 2 - 1 for x in data]\n",
    "print(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02e44b64-17bd-4585-b0b2-a8342a05007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f061500c-d1ff-4305-a720-8742a19d5838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      " [[165  60  25   0 120]\n",
      " [175  70  30   1 130]\n",
      " [155  50  22   0 110]\n",
      " [185  80  35   1 140]\n",
      " [170  65  28   1 125]]\n",
      "Projected dataset (feature extraction using PCA):\n",
      " [[ 1.49066791 -0.68731263  0.1212196 ]\n",
      " [-1.22249242  0.35047746  0.05853314]\n",
      " [ 3.17288974  0.06773034 -0.12213922]\n",
      " [-3.11134022 -0.47700365 -0.09325366]\n",
      " [-0.32972502  0.74610848  0.03564014]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample dataset (replace with your own data)\n",
    "dataset = [\n",
    "    [165, 60, 25, 0, 120],\n",
    "    [175, 70, 30, 1, 130],\n",
    "    [155, 50, 22, 0, 110],\n",
    "    [185, 80, 35, 1, 140],\n",
    "    [170, 65, 28, 1, 125]\n",
    "]\n",
    "\n",
    "# Convert dataset to a NumPy array\n",
    "data = np.array(dataset)\n",
    "\n",
    "# Standardize the data\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data_standardized = (data - mean) / std\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = np.cov(data_standardized, rowvar=False)\n",
    "\n",
    "# Perform PCA\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort eigenvectors by eigenvalues in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Choose top k principal components (e.g., k=3)\n",
    "k = 3\n",
    "top_k_eigenvectors = sorted_eigenvectors[:, :k]\n",
    "\n",
    "# Project data onto top k principal components\n",
    "data_projected = np.dot(data_standardized, top_k_eigenvectors)\n",
    "\n",
    "print(\"Original dataset:\\n\", data)\n",
    "print(\"Projected dataset (feature extraction using PCA):\\n\", data_projected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af13476-dee4-410d-b3d4-91bc1092202e",
   "metadata": {},
   "source": [
    "The decision of how many principal components to retain in PCA depends on several factors, including the specific goals of your analysis, the trade-off between dimensionality reduction and information retention, and the amount of explained variance you find acceptable.\n",
    "\n",
    "Here are a few considerations:\n",
    "\n",
    "Explained Variance: One common approach is to examine the explained variance by each principal component. The proportion of total variance explained by a principal component is given by its corresponding eigenvalue divided by the sum of all eigenvalues. You could plot the cumulative explained variance and choose a number of principal components that collectively explain a sufficiently high percentage of the total variance. A common threshold might be to retain enough principal components to explain, for example, 90% or 95% of the total variance.\n",
    "\n",
    "Domain Knowledge: Consider the domain knowledge and your understanding of the features. If you know that only a few features are likely to be relevant to your analysis or prediction task, you might choose to retain fewer principal components.\n",
    "\n",
    "Model Performance: You can experiment with different numbers of principal components and evaluate the impact on your downstream tasks, such as prediction accuracy. Sometimes, a small number of principal components may capture most of the important patterns in the data, leading to good model performance.\n",
    "\n",
    "Interpretability: If interpretability is important, you might choose to retain a smaller number of principal components that have clear physical or semantic meanings.\n",
    "\n",
    "Computational Efficiency: Reducing the number of features can improve the efficiency of some algorithms, so you might consider retaining a number of principal components that balances information retention with computational cost.\n",
    "\n",
    "Scree Plot: A scree plot displays the eigenvalues in descending order. The point at which the plot levels off can indicate a reasonable number of principal components to retain. The \"elbow\" point is often used as a heuristic.\n",
    "\n",
    "Remember that PCA is a tool for dimensionality reduction, but it comes with a trade-off: you're reducing complexity at the cost of losing some information. It's important to strike a balance between reducing dimensionality and retaining enough information to achieve your analysis or prediction goals.\n",
    "\n",
    "In practice, it's common to start by retaining a larger number of principal components, evaluate the performance and interpretability of your results, and then adjust the number based on your findings. Cross-validation and experimentation can help you determine the optimal number of principal components for your specific problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f6095-d43f-42a4-880d-292cf3df2fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
