{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21eecc16-358e-4ee1-bfe8-b0a4074806f1",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31249f-d3f8-4eb5-bb8e-6d6f2b9e03a6",
   "metadata": {},
   "source": [
    "Q1. The K-Nearest Neighbors (KNN) algorithm is a simple and intuitive machine learning algorithm used for both classification and regression tasks. It operates by finding the K nearest data points to a given input (query point) in a dataset and making predictions based on the majority class (for classification) or the average of the nearest neighbors' values (for regression).\n",
    "\n",
    "Q2. Choosing the value of K in KNN is a critical decision in the algorithm's performance. A small K value (e.g., K=1) can lead to a noisy prediction, while a large K value may lead to oversmoothing. The choice of K depends on the dataset and problem at hand. Typically, you can use techniques like cross-validation to find the optimal K value that provides the best performance.\n",
    "\n",
    "Q3. The main difference between KNN classifier and KNN regressor is in their output. KNN classifier predicts a class label for a new data point, while KNN regressor predicts a continuous numeric value. KNN classifier uses majority voting among the nearest neighbors to assign a class, while KNN regressor computes the mean (or other aggregation) of the target values of the nearest neighbors.\n",
    "\n",
    "Q4. Performance in KNN is measured using various metrics, depending on whether it's a classification or regression task. For classification, common metrics include accuracy, precision, recall, F1 score, and ROC AUC. For regression, common metrics include Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (coefficient of determination).\n",
    "\n",
    "Q5. The \"curse of dimensionality\" in KNN refers to the increased computational and memory requirements as the number of features (dimensions) in the dataset grows. In high-dimensional spaces, distance-based methods like KNN can become less effective because data points become increasingly spread out, making it harder to find meaningful neighbors. Dimensionality reduction techniques may be employed to address this issue.\n",
    "\n",
    "Q6. Handling missing values in KNN can be challenging. You can choose to either impute missing values using techniques like mean or median imputation before applying KNN or modify the distance metric to account for missing values appropriately (e.g., ignoring missing values during distance calculation).\n",
    "\n",
    "Q7. KNN classifier and regressor are suitable for different types of problems. KNN classifier is better for problems where the output is a categorical variable and you want to classify data points into specific classes. KNN regressor is suitable for problems where the output is a continuous variable and you want to predict numeric values.\n",
    "\n",
    "Q8. Strengths of KNN:\n",
    "\n",
    "Simple and easy to implement.\n",
    "Works well for both classification and regression.\n",
    "Non-parametric, meaning it can model complex relationships.\n",
    "Weaknesses of KNN:\n",
    "\n",
    "Sensitive to the choice of K.\n",
    "Computationally expensive for large datasets.\n",
    "Affected by the curse of dimensionality in high-dimensional spaces.\n",
    "To address these weaknesses, you can consider dimensionality reduction, weighted KNN, and efficient data structures like KD-trees or ball trees.\n",
    "\n",
    "Q9. The main difference between Euclidean distance and Manhattan distance in KNN is the way they measure distance between data points. Euclidean distance is the straight-line distance between two points in a Euclidean space, while Manhattan distance is the sum of the absolute differences of their coordinates along each dimension. Euclidean distance is sensitive to the magnitude and scaling of features, while Manhattan distance is more robust in this regard.\n",
    "\n",
    "Q10. Feature scaling plays a crucial role in KNN because it ensures that all features contribute equally to the distance calculation. When features have different scales, those with larger scales can dominate the distance metric. Common scaling methods include z-score standardization (mean-centering and scaling to unit variance) and min-max scaling (scaling features to a specific range, typically [0, 1] or [-1, 1]). Feature scaling helps KNN perform more effectively and prevents one feature from having an undue influence on the results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667c73e-8ad1-4b11-81fc-ff9cb1292e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
